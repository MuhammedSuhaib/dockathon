"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[451],{4982:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>g,frontMatter:()=>c,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"module-04-vision-language-action/intro","title":"Module 4","description":"Vision-Language-Action (VLA) - Overview","source":"@site/docs/module-04-vision-language-action/intro.mdx","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/intro","permalink":"/dockathon/docs/module-04-vision-language-action/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/intro.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 06: Navigation - Nav2 and SLAM","permalink":"/dockathon/docs/module-03-ai-robot-brain/chapter-06-navigation"},"next":{"title":"Chapter 7: Whisper Voice Integration - Voice-to-Action Logic","permalink":"/dockathon/docs/module-04-vision-language-action/chapter-07-whisper"}}');var s=t(2714),a=t(8885),o=t(6251),r=t(9625);const c={sidebar_position:1},l="Module 4",d={},u=[{value:"Vision-Language-Action (VLA) - Overview",id:"vision-language-action-vla---overview",level:2},{value:"Interactive Neural Processing",id:"interactive-neural-processing",level:2},{value:"Test Your Understanding",id:"test-your-understanding",level:2}];function h(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4",children:"Module 4"})}),"\n",(0,s.jsx)(n.h2,{id:"vision-language-action-vla---overview",children:"Vision-Language-Action (VLA) - Overview"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI. This module explores how visual processing, language understanding, and robotic control can be integrated to create intelligent agents capable of interacting naturally with their environment."}),"\n",(0,s.jsx)(n.h2,{id:"interactive-neural-processing",children:"Interactive Neural Processing"}),"\n",(0,s.jsx)(n.p,{children:"Experiment with how VLA systems process multi-modal inputs:"}),"\n",(0,s.jsx)(o.A,{}),"\n",(0,s.jsx)(n.h2,{id:"test-your-understanding",children:"Test Your Understanding"}),"\n",(0,s.jsx)(r.A,{question:"What does VLA stand for in embodied AI?",options:["Vision-Language-Action","Visual-Language-Assistant","Virtual-Learning-Agent","Variable-Logic-Algorithm"],answer:0,explanation:"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI, integrating visual processing, language understanding, and robotic control."})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},6251:(e,n,t)=>{t.d(n,{A:()=>a});var i=t(9378),s=t(2714);const a=()=>{const[e,n]=(0,i.useState)([.5,.3]),[t,a]=(0,i.useState)([.8,.2]),[o,r]=(0,i.useState)(0),[c,l]=(0,i.useState)(!1);(0,i.useEffect)(()=>{const n=e[0]*t[0]+e[1]*t[1];r(parseFloat(n.toFixed(3)))},[e,t]);return(0,s.jsxs)("div",{className:"neural-network-simulator",children:[(0,s.jsx)("h3",{children:"Neural Network Simulator"}),(0,s.jsx)("p",{children:"Adjust inputs and weights to see how they affect the output"}),(0,s.jsxs)("div",{className:"inputs-weights",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{children:"Inputs"}),e.map((t,i)=>(0,s.jsxs)("div",{className:"input-group",children:[(0,s.jsxs)("label",{children:["Input ",i+1,": "]}),(0,s.jsx)("input",{type:"range",min:"0",max:"1",step:"0.1",value:t,onChange:t=>((t,i)=>{const s=[...e];s[t]=parseFloat(i)||0,n(s)})(i,t.target.value)}),(0,s.jsx)("span",{children:t})]},i))]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{children:"Weights"}),t.map((e,n)=>(0,s.jsxs)("div",{className:"weight-group",children:[(0,s.jsxs)("label",{children:["Weight ",n+1,": "]}),(0,s.jsx)("input",{type:"range",min:"0",max:"1",step:"0.1",value:e,onChange:e=>((e,n)=>{const i=[...t];i[e]=parseFloat(n)||0,a(i)})(n,e.target.value)}),(0,s.jsx)("span",{children:e})]},n))]})]}),(0,s.jsxs)("div",{className:"output",children:[(0,s.jsx)("strong",{children:"Output:"})," ",o]}),(0,s.jsx)("p",{className:"formula",children:"Output = (Input\u2081 \xd7 Weight\u2081) + (Input\u2082 \xd7 Weight\u2082)"})]})}},8885:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(9378);const s={},a=i.createContext(s);function o(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}},9625:(e,n,t)=>{t.d(n,{A:()=>a});var i=t(9378),s=t(2714);const a=({question:e,options:n,answer:t,explanation:a})=>{const[o,r]=(0,i.useState)(null),[c,l]=(0,i.useState)(!1),d=o===t;return(0,s.jsxs)("div",{className:"quiz-container",children:[(0,s.jsx)("h3",{children:"\ud83e\udde0 Knowledge Check"}),(0,s.jsxs)("form",{onSubmit:e=>{e.preventDefault(),l(!0)},children:[(0,s.jsx)("p",{children:(0,s.jsx)("strong",{children:e})}),n.map((e,n)=>(0,s.jsx)("div",{className:"options-container",children:(0,s.jsxs)("label",{className:"option-label",children:[(0,s.jsx)("input",{type:"radio",name:"quiz-option",value:n,checked:o===n,onChange:()=>r(n),disabled:c}),e]})},n)),c?(0,s.jsxs)("div",{className:"result-container",children:[(0,s.jsx)("div",{className:"result-feedback "+(d?"correct":"incorrect"),children:d?"\u2705 Correct!":"\u274c Try again!"}),a&&(0,s.jsx)("div",{className:"explanation",children:a})]}):(0,s.jsx)("button",{type:"submit",className:"submit-button",children:"Check Answer"})]})]})}}}]);