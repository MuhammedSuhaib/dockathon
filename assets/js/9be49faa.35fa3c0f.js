"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[706],{676:(e,n,t)=>{t.d(n,{A:()=>a});var o=t(9378),i=t(2714);function a(){const[e,n]=(0,o.useState)(!1),[t,a]=(0,o.useState)(null),s=(e,n,t)=>{if(0===e.children.length)return void(e.textContent=t);const o=document.createTreeWalker(e,NodeFilter.SHOW_TEXT,null),i=[];let a;for(;a=o.nextNode();)i.push(a);i.forEach(e=>{e.textContent?.trim()===n.trim()?e.textContent=t:e.textContent?.includes(n)&&(e.textContent=e.textContent?.replace(n,t)||"")})},r=()=>"undefined"!=typeof window&&window.location&&"localhost"===window.location.hostname?"http://localhost:8000":"https://speckit-plus-production.up.railway.app";return(0,i.jsxs)("div",{className:"translate-button-container",style:{marginBottom:"20px"},children:[(0,i.jsx)("button",{onClick:async()=>{if(!e)if("undefined"!=typeof window){n(!0),a(null);try{const e=document.querySelector("article");if(!e)throw new Error("Article content not found");const t=e.querySelectorAll("div, p, h1, h2, h3, h4, h5, h6, span, li"),o=Array.from(t).map(e=>({element:e,originalText:e.textContent||"",isCode:null!==e.closest("pre, code")})).filter(e=>""!==e.originalText.trim()&&!e.isCode);if(0===o.length)throw new Error("No translatable content found");if(!confirm("This will translate the content to Urdu. Auto-translated content may have errors. Continue?"))return void n(!1);for(const n of o)if(""!==n.originalText.trim()&&!n.isCode){const e=r(),t=await fetch(`${e}/api/translate-text`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:n.originalText,target_language:"ur"})});if(!t.ok)throw new Error(`Translation API error: ${t.status}`);const{translated_text:o}=await t.json();n.element.setAttribute("dir","rtl"),n.element.style.direction="rtl",n.element.style.textAlign="right",n.element.style.fontFamily="Tahoma, Arial, sans-serif",s(n.element,n.originalText,o)}alert("Translation to Urdu completed!")}catch(t){console.error("Translation error:",t),a(t instanceof Error?t.message:"An error occurred during translation")}finally{n(!1)}}else a("Translation is only available in browser environment")},disabled:e,style:{padding:"8px 16px",backgroundColor:e?"#ccc":"#00cc44",color:"white",border:"none",borderRadius:"4px",cursor:e?"not-allowed":"pointer"},children:e?"Translating...":".Translate to Urdu"}),t&&(0,i.jsxs)("div",{style:{color:"red",marginTop:"5px"},children:["Error: ",t]})]})}},6199:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-04-vision-language-action/chapter-09-capstone-project","title":"chapter-09-capstone-project","description":"This capstone project brings together all concepts learned throughout the course to create an autonomous humanoid robot capable of receiving voice commands, planning paths, navigating obstacles, identifying objects using computer vision, and manipulating them. This project demonstrates the integration of ROS 2, Gazebo simulation, NVIDIA Isaac, and Vision-Language-Action (VLA) systems.","source":"@site/docs/module-04-vision-language-action/chapter-09-capstone-project.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/chapter-09-capstone-project","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/chapter-09-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/chapter-09-capstone-project.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"capstone-logic","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/capstone-logic"}}');var i=t(2714),a=t(8885),s=t(676);const r={sidebar_position:6},l="Capstone Project: The Autonomous Humanoid",c={},d=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"System Components Integration",id:"system-components-integration",level:2},{value:"1. Voice Command Processing",id:"1-voice-command-processing",level:3},{value:"2. Path Planning for Bipedal Humanoid Movement",id:"2-path-planning-for-bipedal-humanoid-movement",level:3},{value:"3. Computer Vision for Object Identification",id:"3-computer-vision-for-object-identification",level:3},{value:"4. Manipulation System",id:"4-manipulation-system",level:3},{value:"Integration Pipeline",id:"integration-pipeline",level:2},{value:"1. Voice Command to Action Sequence",id:"1-voice-command-to-action-sequence",level:3},{value:"Simulation Environment",id:"simulation-environment",level:2},{value:"Setting up the Gazebo Environment",id:"setting-up-the-gazebo-environment",level:3},{value:"Isaac Sim Integration",id:"isaac-sim-integration",level:3},{value:"Implementation Workflow",id:"implementation-workflow",level:2},{value:"Phase 1: Individual Component Testing",id:"phase-1-individual-component-testing",level:3},{value:"Phase 2: Component Integration",id:"phase-2-component-integration",level:3},{value:"Phase 3: End-to-End Testing",id:"phase-3-end-to-end-testing",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Functional Requirements",id:"functional-requirements",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Voice Recognition Problems",id:"1-voice-recognition-problems",level:3},{value:"2. Navigation Failures",id:"2-navigation-failures",level:3},{value:"3. Object Manipulation Issues",id:"3-object-manipulation-issues",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Simulation to Real Robot Transfer",id:"simulation-to-real-robot-transfer",level:3},{value:"Safety Measures",id:"safety-measures",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.A,{}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.p,{children:"This capstone project brings together all concepts learned throughout the course to create an autonomous humanoid robot capable of receiving voice commands, planning paths, navigating obstacles, identifying objects using computer vision, and manipulating them. This project demonstrates the integration of ROS 2, Gazebo simulation, NVIDIA Isaac, and Vision-Language-Action (VLA) systems."}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"The Autonomous Humanoid integrates multiple AI and robotics technologies to create a system that can:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Receive and understand natural language voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Plan navigation paths to reach specified locations"}),"\n",(0,i.jsx)(n.li,{children:"Navigate through environments with obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Identify objects using computer vision"}),"\n",(0,i.jsx)(n.li,{children:"Manipulate objects based on the understood command"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate all four modules into a unified system"}),"\n",(0,i.jsx)(n.li,{children:"Implement end-to-end voice-to-action pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Demonstrate multimodal perception and planning"}),"\n",(0,i.jsx)(n.li,{children:"Deploy cognitive planning systems on humanoid robot platforms"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[Audio Input] \u2192 [Whisper] \u2192 [LLM] \u2192 [Task Planner] \u2192 [Navigation] \u2192 [Manipulation]\n     \u2191                                              \u2193\n[Human Command] \u2190 [Human-Robot Interface] \u2190 [ROS 2 Control]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"system-components-integration",children:"System Components Integration"}),"\n",(0,i.jsx)(n.h3,{id:"1-voice-command-processing",children:"1. Voice Command Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rospy\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport openai\n\nclass VoiceCommandProcessor:\n    def __init__(self):\n        # Initialize Whisper model\n        self.whisper_model = whisper.load_model("base")\n        \n        # Initialize ROS node\n        rospy.init_node(\'voice_command_processor\')\n        \n        # Setup subscribers and publishers\n        self.audio_sub = rospy.Subscriber(\'/audio_input\', AudioData, self.audio_callback)\n        self.command_pub = rospy.Publisher(\'/nlp_command\', String, queue_size=10)\n        \n    def audio_callback(self, audio_msg):\n        # Convert audio to text using Whisper\n        audio_array = self.convert_audio_to_array(audio_msg)\n        result = self.whisper_model.transcribe(audio_array)\n        transcription = result["text"]\n        \n        # Send to LLM for semantic understanding\n        nlp_command = self.process_with_llm(transcription)\n        \n        # Publish processed command\n        self.command_pub.publish(String(data=nlp_command))\n    \n    def process_with_llm(self, transcription):\n        # Use LLM to extract semantic intent\n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": "You are a command interpreter for a humanoid robot. Convert natural language commands into structured robot commands. Output format: {\'action\': \'action_type\', \'target\': \'object_or_location\', \'details\': {...}}"},\n                {"role": "user", "content": f"Command: \'{transcription}\'"}\n            ]\n        )\n        return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-path-planning-for-bipedal-humanoid-movement",children:"2. Path Planning for Bipedal Humanoid Movement"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from nav_msgs.msg import OccupancyGrid, Path\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nimport rclpy\nfrom rclpy.action import ActionClient\n\nclass HumanoidPathPlanner:\n    def __init__(self):\n        # Initialize ROS 2 components\n        self.navigator = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        \n    def plan_path(self, start_pose, goal_pose):\n        # Account for humanoid-specific constraints\n        humanoid_constraints = {\n            'max_step_height': 0.1,  # Humanoid maximum step height\n            'max_step_length': 0.3,  # Humanoid maximum step length\n            'foot_separation': 0.2,  # Distance between feet\n            'turning_radius': 0.4    # Minimum turning radius\n        }\n        \n        # Plan path considering bipedal constraints\n        path = self.nav2_path_planner.plan_path_with_constraints(\n            start_pose, \n            goal_pose, \n            humanoid_constraints\n        )\n        \n        return path\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-computer-vision-for-object-identification",children:"3. Computer Vision for Object Identification"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport torch\nfrom yolov5 import detect\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass HumanoidVisionSystem:\n    def __init__(self):\n        # Load YOLO model for object detection\n        self.yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n        self.cv_bridge = CvBridge()\n        \n    def detect_objects(self, image_msg):\n        # Convert ROS image to OpenCV format\n        cv_image = self.cv_bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\n        \n        # Run object detection\n        results = self.yolo_model(cv_image)\n        \n        # Extract relevant objects\n        detections = []\n        for detection in results.xyxy[0]:\n            x1, y1, x2, y2, conf, cls = detection\n            if conf > 0.5:  # Confidence threshold\n                object_info = {\n                    'class': self.yolo_model.names[int(cls)],\n                    'confidence': conf,\n                    'bbox': [float(x1), float(y1), float(x2), float(y2)],\n                    'center': [(x1+x2)/2, (y1+y2)/2]\n                }\n                detections.append(object_info)\n        \n        return detections\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-manipulation-system",children:"4. Manipulation System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from control_msgs.msg import JointTrajectoryControllerState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport numpy as np\n\nclass HumanoidManipulator:\n    def __init__(self):\n        # Initialize manipulator controllers\n        self.arm_controller = rospy.Publisher('/arm_controller/command', JointTrajectory, queue_size=1)\n        self.gripper_controller = rospy.Publisher('/gripper_controller/command', JointTrajectory, queue_size=1)\n        \n    def grasp_object(self, object_pose, approach_angle=0.0):\n        # Plan approach trajectory\n        approach_traj = self.plan_approach_trajectory(object_pose, approach_angle)\n        \n        # Execute approach\n        self.arm_controller.publish(approach_traj)\n        \n        # Close gripper\n        grip_traj = self.create_gripper_close_trajectory()\n        self.gripper_controller.publish(grip_traj)\n        \n        # Lift object\n        lift_traj = self.plan_lift_trajectory()\n        self.arm_controller.publish(lift_traj)\n        \n    def place_object(self, target_pose):\n        # Move to target position\n        move_traj = self.plan_move_trajectory(target_pose)\n        self.arm_controller.publish(move_traj)\n        \n        # Open gripper to release\n        release_traj = self.create_gripper_open_trajectory()\n        self.gripper_controller.publish(release_traj)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-pipeline",children:"Integration Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"1-voice-command-to-action-sequence",children:"1. Voice Command to Action Sequence"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AutonomousHumanoidSystem:\n    def __init__(self):\n        self.voice_processor = VoiceCommandProcessor()\n        self.path_planner = HumanoidPathPlanner()\n        self.vision_system = HumanoidVisionSystem()\n        self.manipulator = HumanoidManipulator()\n        self.ros_interface = ROS2Interface()\n        \n    def execute_voice_command(self, command):\n        # Step 1: Parse the voice command\n        structured_command = self.voice_processor.process_with_llm(command)\n        \n        # Step 2: Identify required actions\n        if structured_command.action == "fetch_and_place":\n            self.execute_fetch_and_place(structured_command)\n        elif structured_command.action == "navigate_and_identify":\n            self.execute_navigation(structured_command)\n        # Add more action types as needed\n        \n    def execute_fetch_and_place(self, command):\n        # 1. Find the object to fetch\n        target_object = self.find_target_object(command.target)\n        \n        # 2. Navigate to the object\n        self.navigate_to_object(target_object.location)\n        \n        # 3. Identify and grasp the object\n        self.grasp_object(target_object)\n        \n        # 4. Navigate to destination\n        self.navigate_to_destination(command.destination)\n        \n        # 5. Place the object\n        self.place_object(command.destination)\n    \n    def find_target_object(self, object_description):\n        # Use vision system to locate the described object\n        # Return object location and properties\n        pass\n        \n    def navigate_to_object(self, object_location):\n        # Plan and execute navigation to object\n        current_pose = self.ros_interface.get_current_pose()\n        path = self.path_planner.plan_path(current_pose, object_location)\n        self.ros_interface.execute_path(path)\n        \n    def navigate_to_destination(self, destination):\n        # Plan and execute navigation to destination\n        current_pose = self.ros_interface.get_current_pose()\n        path = self.path_planner.plan_path(current_pose, destination)\n        self.ros_interface.execute_path(path)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"simulation-environment",children:"Simulation Environment"}),"\n",(0,i.jsx)(n.h3,{id:"setting-up-the-gazebo-environment",children:"Setting up the Gazebo Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- Example world file for humanoid simulation --\x3e\n<sdf version='1.7'>\n  <world name='humanoid_world'>\n    \x3c!-- Add room environment --\x3e\n    <include>\n      <uri>model://room_with_furniture</uri>\n      <pose>0 0 0 0 0 0</pose>\n    </include>\n    \n    \x3c!-- Humanoid robot --\x3e\n    <include>\n      <uri>model://humanoid_robot</uri>\n      <pose>0 0 0.8 0 0 0</pose>\n    </include>\n    \n    \x3c!-- Objects to manipulate --\x3e\n    <include>\n      <uri>model://object_to_clean</uri>\n      <pose>-1 1 0.8 0 0 0</pose>\n    </include>\n  </world>\n</sdf>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-sim-integration",children:"Isaac Sim Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class IsaacSimIntegration:\n    def __init__(self):\n        # Initialize Isaac Sim\n        self.gym = gymapi.acquire_gym()\n        self.sim = self.gym.create_sim(0, 0, gymapi.SIM_PHYSX)\n        \n    def setup_humanoid_env(self):\n        # Create humanoid robot in Isaac Sim\n        # Configure sensors (camera, IMU, etc.)\n        # Set up physics properties for bipedal locomotion\n        pass\n        \n    def run_pegasus_locomotion(self, humanoid_actor):\n        # Implement humanoid locomotion\n        # Balance control for bipedal movement\n        # Footstep planning\n        pass\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-workflow",children:"Implementation Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"phase-1-individual-component-testing",children:"Phase 1: Individual Component Testing"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Test voice recognition and LLM command interpretation"}),"\n",(0,i.jsx)(n.li,{children:"Test path planning with humanoid constraints"}),"\n",(0,i.jsx)(n.li,{children:"Test object detection and identification"}),"\n",(0,i.jsx)(n.li,{children:"Test basic manipulation movements"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"phase-2-component-integration",children:"Phase 2: Component Integration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate voice processing with command planning"}),"\n",(0,i.jsx)(n.li,{children:"Connect navigation system with object detection"}),"\n",(0,i.jsx)(n.li,{children:"Link manipulation system to visual feedback"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"phase-3-end-to-end-testing",children:"Phase 3: End-to-End Testing"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Test complete voice command to execution sequence"}),"\n",(0,i.jsx)(n.li,{children:"Handle failure cases and error recovery"}),"\n",(0,i.jsx)(n.li,{children:"Optimize performance for real-time operation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsx)(n.h3,{id:"functional-requirements",children:"Functional Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Successfully receive and interpret voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Navigate through environments with obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Identify and manipulate requested objects"}),"\n",(0,i.jsx)(n.li,{children:"Complete tasks with minimal human intervention"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Understanding Accuracy"}),": Percentage of commands correctly interpreted"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation Success Rate"}),": Percentage of successful navigations to targets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Manipulation Success"}),": Percentage of successful grasps and placements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Response Time"}),": Average time from command to action initiation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"1-voice-recognition-problems",children:"1. Voice Recognition Problems"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ensure proper microphone setup and audio quality"}),"\n",(0,i.jsx)(n.li,{children:"Adjust Whisper model parameters for environment"}),"\n",(0,i.jsx)(n.li,{children:"Implement voice activity detection"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-navigation-failures",children:"2. Navigation Failures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Verify map accuracy and obstacle detection"}),"\n",(0,i.jsx)(n.li,{children:"Adjust humanoid-specific navigation parameters"}),"\n",(0,i.jsx)(n.li,{children:"Implement fallback navigation strategies"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-object-manipulation-issues",children:"3. Object Manipulation Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Calibrate vision system to robotic arm"}),"\n",(0,i.jsx)(n.li,{children:"Implement grasp pose estimation"}),"\n",(0,i.jsx)(n.li,{children:"Add tactile feedback for grasp confirmation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"simulation-to-real-robot-transfer",children:"Simulation to Real Robot Transfer"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Validate simulation accuracy against real-world behavior"}),"\n",(0,i.jsx)(n.li,{children:"Account for sim-to-real domain differences"}),"\n",(0,i.jsx)(n.li,{children:"Implement adaptive control for real-world variations"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"safety-measures",children:"Safety Measures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement emergency stop mechanisms"}),"\n",(0,i.jsx)(n.li,{children:"Ensure robot operates within safe joint limits"}),"\n",(0,i.jsx)(n.li,{children:"Add collision avoidance during manipulation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"The Autonomous Humanoid capstone project demonstrates the integration of all course modules:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 1"}),": ROS 2 for robot control and communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 2"}),": Gazebo simulation and Unity visualization for testing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 3"}),": NVIDIA Isaac for advanced perception and navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 4"}),": VLA systems for cognitive planning and voice interaction"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This project showcases how embodied intelligence bridges the gap between digital AI and physical robotic systems, creating robots that can understand and act on natural human commands in real-world environments."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8885:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(9378);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);