"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[38],{2842:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Robotic Nervous System","items":[{"type":"link","href":"/dockathon/docs/module-01-robotic-nervous-system/intro","label":"Module 1","docId":"module-01-robotic-nervous-system/intro","unlisted":false},{"type":"link","href":"/dockathon/docs/module-01-robotic-nervous-system/chapter-01-ros2-basics","label":"Nodes, Topics, Services","docId":"module-01-robotic-nervous-system/chapter-01-ros2-basics","unlisted":false},{"type":"link","href":"/dockathon/docs/module-01-robotic-nervous-system/chapter-02-hardware","label":"Setting up Jetson Orin and RealSense","docId":"module-01-robotic-nervous-system/chapter-02-hardware","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Digital Twin","items":[{"type":"link","href":"/dockathon/docs/module-02-digital-twin/intro","label":"Module 2","docId":"module-02-digital-twin/intro","unlisted":false},{"type":"link","href":"/dockathon/docs/module-02-digital-twin/chapter-03-gazebo-urdf","label":"Building a robot body in XML (URDF)","docId":"module-02-digital-twin/chapter-03-gazebo-urdf","unlisted":false},{"type":"link","href":"/dockathon/docs/module-02-digital-twin/chapter-04-isaac-sim","label":"Intro to NVIDIA Omniverse & USD","docId":"module-02-digital-twin/chapter-04-isaac-sim","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"AI Robot Brain","items":[{"type":"link","href":"/dockathon/docs/module-03-ai-robot-brain/intro","label":"Module 3","docId":"module-03-ai-robot-brain/intro","unlisted":false},{"type":"link","href":"/dockathon/docs/module-03-ai-robot-brain/chapter-05-computer-vision","label":"Chapter 05: Computer Vision - YOLO and Depth Cameras","docId":"module-03-ai-robot-brain/chapter-05-computer-vision","unlisted":false},{"type":"link","href":"/dockathon/docs/module-03-ai-robot-brain/chapter-06-navigation","label":"Chapter 06: Navigation - Nav2 and SLAM","docId":"module-03-ai-robot-brain/chapter-06-navigation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Vision Language Action","items":[{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/intro","label":"Module 4","docId":"module-04-vision-language-action/intro","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/chapter-07-whisper","label":"Chapter 7: Whisper Voice Integration - Voice-to-Action Logic","docId":"module-04-vision-language-action/chapter-07-whisper","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/whisper-integration","label":"Whisper Voice Integration","docId":"module-04-vision-language-action/whisper-integration","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/vla-models","label":"VLA Models (Vision-Language-Action)","docId":"module-04-vision-language-action/vla-models","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/capstone-logic","label":"Capstone Logic: Complete VLA System Implementation","docId":"module-04-vision-language-action/capstone-logic","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/","label":"Module 4: Vision-Language-Action (VLA) Models","docId":"module-04-vision-language-action/README","unlisted":false},{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/chapter-08-vla-models","label":"chapter-08-vla-models","docId":"module-04-vision-language-action/chapter-08-vla-models","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module-01-robotic-nervous-system/chapter-01-ros2-basics":{"id":"module-01-robotic-nervous-system/chapter-01-ros2-basics","title":"Nodes, Topics, Services","description":"ROS 2 (Robot Operating System 2) provides the communication framework for robotic applications. Understanding its core concepts of nodes, topics, and services is essential for building distributed robotic systems.","sidebar":"tutorialSidebar"},"module-01-robotic-nervous-system/chapter-02-hardware":{"id":"module-01-robotic-nervous-system/chapter-02-hardware","title":"Setting up Jetson Orin and RealSense","description":"The Jetson Orin and Intel RealSense cameras form a powerful combination for robotic applications, providing high-performance computing and accurate depth perception capabilities.","sidebar":"tutorialSidebar"},"module-01-robotic-nervous-system/intro":{"id":"module-01-robotic-nervous-system/intro","title":"Module 1","description":"From Digital AI to Physical AI","sidebar":"tutorialSidebar"},"module-02-digital-twin/chapter-03-gazebo-urdf":{"id":"module-02-digital-twin/chapter-03-gazebo-urdf","title":"Building a robot body in XML (URDF)","description":"Unified Robot Description Format (URDF) is an XML-based format used to describe robotic systems in ROS. It defines the physical and visual properties of a robot, including links, joints, and their relationships.","sidebar":"tutorialSidebar"},"module-02-digital-twin/chapter-04-isaac-sim":{"id":"module-02-digital-twin/chapter-04-isaac-sim","title":"Intro to NVIDIA Omniverse & USD","description":"NVIDIA Isaac Sim, built on the Omniverse platform, provides a powerful simulation environment for robotics development. It uses Universal Scene Description (USD) as its core data format, enabling high-fidelity physics simulation and photorealistic rendering.","sidebar":"tutorialSidebar"},"module-02-digital-twin/intro":{"id":"module-02-digital-twin/intro","title":"Module 2","description":"Why we simulate","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-05-computer-vision":{"id":"module-03-ai-robot-brain/chapter-05-computer-vision","title":"Chapter 05: Computer Vision - YOLO and Depth Cameras","description":"Introduction to Computer Vision in Robotics","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-06-navigation":{"id":"module-03-ai-robot-brain/chapter-06-navigation","title":"Chapter 06: Navigation - Nav2 and SLAM","description":"Introduction to Robot Navigation","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/intro":{"id":"module-03-ai-robot-brain/intro","title":"Module 3","description":"AI Robot Brain - Perception Overview","sidebar":"tutorialSidebar"},"module-04-vision-language-action/capstone-logic":{"id":"module-04-vision-language-action/capstone-logic","title":"Capstone Logic: Complete VLA System Implementation","description":"The capstone logic represents the integration and orchestration of all components in a Vision-Language-Action (VLA) system. This section covers the end-to-end implementation and deployment considerations for complete VLA applications.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/chapter-07-whisper":{"id":"module-04-vision-language-action/chapter-07-whisper","title":"Chapter 7: Whisper Voice Integration - Voice-to-Action Logic","description":"This chapter explores the integration of OpenAI Whisper for speech recognition combined with ROS 2 for robotic control, creating a voice-to-action pipeline that enables natural human-robot interaction.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/chapter-08-vla-models":{"id":"module-04-vision-language-action/chapter-08-vla-models","title":"chapter-08-vla-models","description":"","sidebar":"tutorialSidebar"},"module-04-vision-language-action/intro":{"id":"module-04-vision-language-action/intro","title":"Module 4","description":"Vision-Language-Action (VLA) - Overview","sidebar":"tutorialSidebar"},"module-04-vision-language-action/README":{"id":"module-04-vision-language-action/README","title":"Module 4: Vision-Language-Action (VLA) Models","description":"This directory contains the complete documentation for Module 4","sidebar":"tutorialSidebar"},"module-04-vision-language-action/vla-models":{"id":"module-04-vision-language-action/vla-models","title":"VLA Models (Vision-Language-Action)","description":"Vision-Language-Action (VLA) models are multimodal neural networks that jointly process visual observations, natural language instructions, and produce robotic actions. These models represent a breakthrough in embodied AI, enabling robots to interact with the world through a combination of perception, language understanding, and action execution.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/whisper-integration":{"id":"module-04-vision-language-action/whisper-integration","title":"Whisper Voice Integration","description":"Whisper is an open-source automatic speech recognition (ASR) system developed by OpenAI. It is designed to handle multiple languages and various speech recognition tasks, making it ideal for voice integration in embodied intelligence systems.","sidebar":"tutorialSidebar"}}}}')}}]);