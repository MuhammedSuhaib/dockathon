"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[506],{676:(e,t,n)=>{n.d(t,{A:()=>r});var i=n(9378),o=n(2714);function r(){const[e,t]=(0,i.useState)(!1),[n,r]=(0,i.useState)(null),a=(e,t,n)=>{if(0===e.children.length)return void(e.textContent=n);const i=document.createTreeWalker(e,NodeFilter.SHOW_TEXT,null),o=[];let r;for(;r=i.nextNode();)o.push(r);o.forEach(e=>{e.textContent?.trim()===t.trim()?e.textContent=n:e.textContent?.includes(t)&&(e.textContent=e.textContent?.replace(t,n)||"")})};return(0,o.jsxs)("div",{className:"translate-button-container",style:{marginBottom:"20px"},children:[(0,o.jsx)("button",{onClick:async()=>{if(!e)if("undefined"!=typeof window){t(!0),r(null);try{const e=document.querySelector("article");if(!e)throw new Error("Article content not found");const n=e.querySelectorAll("div, p, h1, h2, h3, h4, h5, h6, span, li"),i=Array.from(n).map(e=>({element:e,originalText:e.textContent||"",isCode:null!==e.closest("pre, code")})).filter(e=>""!==e.originalText.trim()&&!e.isCode);if(0===i.length)throw new Error("No translatable content found");if(!confirm("This will translate the content to Urdu. Auto-translated content may have errors. Continue?"))return void t(!1);for(const t of i)if(""!==t.originalText.trim()&&!t.isCode){const e=await fetch("http://localhost:8000/api/translate-text",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t.originalText,target_language:"ur"})});if(!e.ok)throw new Error(`Translation API error: ${e.status}`);const{translated_text:n}=await e.json();t.element.setAttribute("dir","rtl"),t.element.style.direction="rtl",t.element.style.textAlign="right",t.element.style.fontFamily="Tahoma, Arial, sans-serif",a(t.element,t.originalText,n)}alert("Translation to Urdu completed!")}catch(n){console.error("Translation error:",n),r(n instanceof Error?n.message:"An error occurred during translation")}finally{t(!1)}}else r("Translation is only available in browser environment")},disabled:e,style:{padding:"8px 16px",backgroundColor:e?"#ccc":"#00cc44",color:"white",border:"none",borderRadius:"4px",cursor:e?"not-allowed":"pointer"},children:e?"Translating...":".Translate to Urdu"}),n&&(0,o.jsxs)("div",{style:{color:"red",marginTop:"5px"},children:["Error: ",n]})]})}},7510:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-03-ai-robot-brain/chapter-05-computer-vision","title":"chapter-05-computer-vision","description":"Introduction to Computer Vision in Robotics","source":"@site/docs/module-03-ai-robot-brain/chapter-05-computer-vision.md","sourceDirName":"module-03-ai-robot-brain","slug":"/module-03-ai-robot-brain/chapter-05-computer-vision","permalink":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-05-computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-ai-robot-brain/chapter-05-computer-vision.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"intro","permalink":"/SpecKit-Plus/docs/module-03-ai-robot-brain/intro"},"next":{"title":"chapter-06-navigation","permalink":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-06-navigation"}}');var o=n(2714),r=n(8885),a=n(676);const s={sidebar_position:2},l="Computer Vision - YOLO and Depth Cameras",c={},d=[{value:"Introduction to Computer Vision in Robotics",id:"introduction-to-computer-vision-in-robotics",level:2},{value:"YOLO (You Only Look Once) for Real-time Object Detection",id:"yolo-you-only-look-once-for-real-time-object-detection",level:2},{value:"How YOLO Works",id:"how-yolo-works",level:3},{value:"YOLO in Robotic Applications",id:"yolo-in-robotic-applications",level:3},{value:"Depth Cameras for Spatial Perception",id:"depth-cameras-for-spatial-perception",level:2},{value:"Types of Depth Cameras",id:"types-of-depth-cameras",level:3},{value:"Depth Camera Applications in Robotics",id:"depth-camera-applications-in-robotics",level:3},{value:"Integration with Robotic Systems",id:"integration-with-robotic-systems",level:2},{value:"Object Detection with Depth Information",id:"object-detection-with-depth-information",level:3},{value:"ROS Integration",id:"ros-integration",level:3},{value:"Best Practices for Implementation",id:"best-practices-for-implementation",level:2},{value:"Summary",id:"summary",level:2}];function h(e){const t={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.A,{}),"\n",(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"computer-vision---yolo-and-depth-cameras",children:"Computer Vision - YOLO and Depth Cameras"})}),"\n",(0,o.jsx)(t.h2,{id:"introduction-to-computer-vision-in-robotics",children:"Introduction to Computer Vision in Robotics"}),"\n",(0,o.jsx)(t.p,{children:"Computer vision is a critical component of robotic perception systems, enabling robots to interpret and understand their visual environment. This chapter explores two fundamental technologies in computer vision: object detection using YOLO (You Only Look Once) and spatial perception using depth cameras."}),"\n",(0,o.jsx)(t.h2,{id:"yolo-you-only-look-once-for-real-time-object-detection",children:"YOLO (You Only Look Once) for Real-time Object Detection"}),"\n",(0,o.jsx)(t.p,{children:"YOLO is a state-of-the-art real-time object detection system that revolutionized computer vision applications in robotics. Unlike traditional methods that process images in multiple passes, YOLO performs object detection in a single forward pass through a neural network, making it suitable for robotic applications where speed is critical."}),"\n",(0,o.jsx)(t.h3,{id:"how-yolo-works",children:"How YOLO Works"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Single-Pass Detection"}),": YOLO divides the input image into a grid and simultaneously predicts bounding boxes and class probabilities for each grid cell"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Speed vs. Accuracy"}),": YOLO provides a good balance between detection speed and accuracy, making it ideal for robotic applications"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Real-time Processing"}),": Modern versions of YOLO (YOLOv5, YOLOv8) can achieve real-time performance on standard hardware"]}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"yolo-in-robotic-applications",children:"YOLO in Robotic Applications"}),"\n",(0,o.jsx)(t.p,{children:"In robotics, YOLO is commonly used for:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Object recognition and classification in the robot's environment"}),"\n",(0,o.jsx)(t.li,{children:"Human detection for collaborative robotics"}),"\n",(0,o.jsx)(t.li,{children:"Obstacle detection for navigation systems"}),"\n",(0,o.jsx)(t.li,{children:"Quality inspection in manufacturing robotics"}),"\n",(0,o.jsx)(t.li,{children:"Agricultural robotics for crop monitoring and harvesting"}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"depth-cameras-for-spatial-perception",children:"Depth Cameras for Spatial Perception"}),"\n",(0,o.jsx)(t.p,{children:"Depth cameras provide crucial 3D spatial information that is essential for robotic navigation, manipulation, and interaction with the environment. These cameras capture not only color information but also depth data for each pixel in the image."}),"\n",(0,o.jsx)(t.h3,{id:"types-of-depth-cameras",children:"Types of Depth Cameras"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Stereo Vision Cameras"}),": Use two or more cameras to capture images from slightly different angles to calculate depth based on parallax"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Time-of-Flight (ToF) Cameras"}),": Measure the time it takes for light to travel to objects and back to determine distance"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Structured Light Cameras"}),": Project a known light pattern and measure how it deforms when hitting surfaces to calculate depth"]}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"depth-camera-applications-in-robotics",children:"Depth Camera Applications in Robotics"}),"\n",(0,o.jsx)(t.p,{children:"Depth cameras enable several key robotic capabilities:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"3D Mapping"}),": Creating detailed spatial maps of the environment"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Obstacle Avoidance"}),": Detecting and avoiding obstacles in the path of mobile robots"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Object Manipulation"}),": Providing 3D information necessary for precise robotic manipulation tasks"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"SLAM Integration"}),": Feeding depth data into Simultaneous Localization and Mapping systems"]}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"integration-with-robotic-systems",children:"Integration with Robotic Systems"}),"\n",(0,o.jsx)(t.p,{children:"Modern robotic systems often integrate YOLO and depth camera technologies to create comprehensive perception capabilities:"}),"\n",(0,o.jsx)(t.h3,{id:"object-detection-with-depth-information",children:"Object Detection with Depth Information"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Combining YOLO object detection with depth data to understand not just what objects are present, but also their 3D positions"}),"\n",(0,o.jsx)(t.li,{children:"Enabling robots to grasp objects at known distances or avoid obstacles based on both recognition and spatial information"}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"ros-integration",children:"ROS Integration"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Both YOLO and depth cameras can be integrated with Robot Operating System (ROS) using standard packages"}),"\n",(0,o.jsx)(t.li,{children:"YOLO implementations are available through ROS packages like darknet_ros"}),"\n",(0,o.jsx)(t.li,{children:"Depth cameras typically provide depth images through ROS sensor_msgs/Image topics with appropriate calibration parameters"}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"best-practices-for-implementation",children:"Best Practices for Implementation"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Lighting Considerations"}),": Different depth camera technologies perform differently under varying lighting conditions"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Computational Requirements"}),": Ensure sufficient computing power for real-time YOLO processing and depth data handling"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Calibration"}),": Properly calibrate both YOLO models (for accuracy) and depth cameras (for spatial accuracy)"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Data Fusion"}),": Implement effective fusion techniques to combine YOLO detection results with depth information"]}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(t.p,{children:"Computer vision combining YOLO object detection with depth camera spatial information creates powerful perception capabilities for robots. These technologies enable robots to understand both what objects are in their environment and where those objects are located in 3D space, which is essential for navigation, manipulation, and interaction tasks."})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8885:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>s});var i=n(9378);const o={},r=i.createContext(o);function a(e){const t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);