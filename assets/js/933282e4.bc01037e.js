"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[773],{8078:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-04-vision-language-action/capstone-logic","title":"Capstone Logic: Complete VLA System Implementation","description":"The capstone logic represents the integration and orchestration of all components in a Vision-Language-Action (VLA) system. This section covers the end-to-end implementation and deployment considerations for complete VLA applications.","source":"@site/docs/module-04-vision-language-action/capstone-logic.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/capstone-logic","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/capstone-logic","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/capstone-logic.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"VLA Models (Vision-Language-Action)","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/vla-models"},"next":{"title":"Module 4: Vision-Language-Action (VLA) Models","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/"}}');var s=i(2714),a=i(8885);const o={sidebar_position:4},l="Capstone Logic: Complete VLA System Implementation",r={},c=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Core Components Integration",id:"core-components-integration",level:2},{value:"1. Input Processing Pipeline",id:"1-input-processing-pipeline",level:3},{value:"2. Modal Fusion Engine",id:"2-modal-fusion-engine",level:3},{value:"3. Action Generation and Execution",id:"3-action-generation-and-execution",level:3},{value:"State Management and Planning",id:"state-management-and-planning",level:2},{value:"State Representation",id:"state-representation",level:3},{value:"Hierarchical Planning",id:"hierarchical-planning",level:3},{value:"Real-time Execution Framework",id:"real-time-execution-framework",level:2},{value:"Event-Driven Architecture",id:"event-driven-architecture",level:3},{value:"Safety and Validation Layer",id:"safety-and-validation-layer",level:2},{value:"Safety Checks",id:"safety-checks",level:3},{value:"Fail-Safe Mechanisms",id:"fail-safe-mechanisms",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Resource Management",id:"resource-management",level:3},{value:"Latency Reduction Strategies",id:"latency-reduction-strategies",level:3},{value:"System Monitoring and Logging",id:"system-monitoring-and-logging",level:2},{value:"Real-time Monitoring",id:"real-time-monitoring",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Environmental Adaptations",id:"environmental-adaptations",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Simulation Testing",id:"simulation-testing",level:3},{value:"Real-World Validation",id:"real-world-validation",level:3},{value:"Example Complete Integration",id:"example-complete-integration",level:2},{value:"Evaluation and Improvement",id:"evaluation-and-improvement",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Continuous Improvement",id:"continuous-improvement",level:3},{value:"Deployment Pipeline",id:"deployment-pipeline",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"capstone-logic-complete-vla-system-implementation",children:"Capstone Logic: Complete VLA System Implementation"})}),"\n",(0,s.jsx)(n.p,{children:"The capstone logic represents the integration and orchestration of all components in a Vision-Language-Action (VLA) system. This section covers the end-to-end implementation and deployment considerations for complete VLA applications."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The capstone logic brings together the individual components\u2014Whisper voice integration, VLA models, and supporting infrastructure\u2014into a cohesive system that can perceive, understand, and act in real-world environments. This system serves as the foundation for practical embodied intelligence applications."}),"\n",(0,s.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA Capstone System                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Input Layer    \u2502 Processing Layer  \u2502 Output Layer          \u2502\n\u2502                \u2502                  \u2502                       \u2502\n\u2502  \u2022 Cameras     \u2502  \u2022 Vision        \u2502  \u2022 Robot Actions      \u2502\n\u2502  \u2022 Microphones \u2502  \u2022 Language      \u2502  \u2022 Text Responses     \u2502\n\u2502  \u2022 Sensors     \u2502  \u2022 VLA Model     \u2502  \u2022 Visual Feedback    \u2502\n\u2502  \u2022 Network     \u2502  \u2022 Reasoning     \u2502  \u2022 Audio Responses    \u2502\n\u2502                \u2502  \u2022 Planning      \u2502                       \u2502\n\u2502                \u2502  \u2022 Control       \u2502                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"core-components-integration",children:"Core Components Integration"}),"\n",(0,s.jsx)(n.h3,{id:"1-input-processing-pipeline",children:"1. Input Processing Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The input processing pipeline manages data ingestion from various sensors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class InputProcessor:\n    def __init__(self):\n        self.camera_processor = CameraProcessor()\n        self.audio_processor = AudioProcessor()\n        self.sensor_processor = SensorProcessor()\n        \n    def collect_multimodal_input(self):\n        # Simultaneously collect visual, audio, and sensor data\n        visual_data = self.camera_processor.get_frame()\n        audio_data = self.audio_processor.get_audio_sample()\n        sensor_data = self.sensor_processor.get_sensor_readings()\n        \n        return {\n            'visual': visual_data,\n            'audio': audio_data,\n            'sensors': sensor_data\n        }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-modal-fusion-engine",children:"2. Modal Fusion Engine"}),"\n",(0,s.jsx)(n.p,{children:"The fusion engine combines information from different modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ModalFusionEngine:\n    def __init__(self):\n        self.visual_encoder = load_vla_visual_encoder()\n        self.language_encoder = load_vla_language_encoder()\n        self.fusion_network = FusionNetwork()\n        \n    def fuse_modalities(self, visual_input, language_input, sensor_input):\n        # Encode visual information\n        visual_features = self.visual_encoder(visual_input)\n        \n        # Encode language command\n        language_features = self.language_encoder(language_input)\n        \n        # Incorporate sensor data\n        fused_features = self.fusion_network(\n            visual_features, \n            language_features, \n            sensor_input\n        )\n        \n        return fused_features\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-action-generation-and-execution",children:"3. Action Generation and Execution"}),"\n",(0,s.jsx)(n.p,{children:"The action generator translates fused representations into robotic commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ActionGenerator:\n    def __init__(self):\n        self.action_decoder = load_vla_action_decoder()\n        self.robot_controller = RobotController()\n        \n    def generate_and_execute(self, fused_features):\n        # Generate action sequence\n        action_sequence = self.action_decoder(fused_features)\n        \n        # Execute actions on robot\n        execution_result = self.robot_controller.execute(\n            action_sequence\n        )\n        \n        return execution_result\n"})}),"\n",(0,s.jsx)(n.h2,{id:"state-management-and-planning",children:"State Management and Planning"}),"\n",(0,s.jsx)(n.h3,{id:"state-representation",children:"State Representation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class StateManager:\n    def __init__(self):\n        self.current_state = {}\n        self.goal_state = {}\n        self.task_history = []\n        \n    def update_state(self, perception_data):\n        # Update internal state based on perception\n        self.current_state.update(perception_data)\n        \n    def plan_sequence(self, goal_description):\n        # Plan action sequence to reach goal\n        plan = self.generate_plan(\n            self.current_state,\n            self.parse_goal(goal_description)\n        )\n        return plan\n"})}),"\n",(0,s.jsx)(n.h3,{id:"hierarchical-planning",children:"Hierarchical Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-level planning"}),": Task decomposition and goal setting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mid-level planning"}),": Subtask sequencing and resource management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low-level planning"}),": Fine-grained motion and control"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-time-execution-framework",children:"Real-time Execution Framework"}),"\n",(0,s.jsx)(n.h3,{id:"event-driven-architecture",children:"Event-Driven Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class VLAExecutionFramework:\n    def __init__(self):\n        self.input_processor = InputProcessor()\n        self.modal_fusion = ModalFusionEngine()\n        self.action_generator = ActionGenerator()\n        self.state_manager = StateManager()\n        self.event_queue = asyncio.Queue()\n        \n    async def run_vla_cycle(self):\n        while True:\n            # Collect inputs\n            inputs = await self.input_processor.collect_multimodal_input()\n            \n            # Process through VLA pipeline\n            fused_features = await self.modal_fusion.fuse_modalities(\n                inputs['visual'],\n                inputs['audio'], \n                inputs['sensors']\n            )\n            \n            # Generate and execute actions\n            result = await self.action_generator.generate_and_execute(\n                fused_features\n            )\n            \n            # Update state and handle events\n            self.state_manager.update_state(result.perception_feedback)\n            await self.handle_events(result.events)\n            \n            await asyncio.sleep(0.01)  # 100Hz control loop\n"})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-validation-layer",children:"Safety and Validation Layer"}),"\n",(0,s.jsx)(n.h3,{id:"safety-checks",children:"Safety Checks"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SafetyValidator:\n    def __init__(self):\n        self.collision_predictor = CollisionPredictor()\n        self.impact_evaluator = ImpactEvaluator()\n        \n    def validate_action(self, action, current_state):\n        # Check for potential collisions\n        collision_risk = self.collision_predictor.predict(\n            action, current_state\n        )\n        \n        # Evaluate potential impact of action\n        impact_assessment = self.impact_evaluator.evaluate(\n            action, current_state\n        )\n        \n        # Return validation result\n        return {\n            'safe': collision_risk < self.safety_threshold and \n                    impact_assessment < self.impact_threshold,\n            'risk_score': max(collision_risk, impact_assessment),\n            'suggested_alternatives': []\n        }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"fail-safe-mechanisms",children:"Fail-Safe Mechanisms"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Emergency stop protocols"}),"\n",(0,s.jsx)(n.li,{children:"Safe position recovery"}),"\n",(0,s.jsx)(n.li,{children:"Graceful degradation when components fail"}),"\n",(0,s.jsx)(n.li,{children:"Redundant sensor validation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU allocation"}),": Dynamic allocation for vision and VLA model inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory management"}),": Efficient caching of model states and precomputed values"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computation scheduling"}),": Prioritizing critical real-time operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"latency-reduction-strategies",children:"Latency Reduction Strategies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model quantization for faster inference"}),"\n",(0,s.jsx)(n.li,{children:"Pipeline parallelism for multi-modal processing"}),"\n",(0,s.jsx)(n.li,{children:"Predictive caching based on recent interactions"}),"\n",(0,s.jsx)(n.li,{children:"Edge computing deployment"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"system-monitoring-and-logging",children:"System Monitoring and Logging"}),"\n",(0,s.jsx)(n.h3,{id:"real-time-monitoring",children:"Real-time Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SystemMonitor:\n    def __init__(self):\n        self.metrics_collector = MetricsCollector()\n        self.health_checker = HealthChecker()\n        self.log_manager = LogManager()\n        \n    def monitor_system(self):\n        # Collect performance metrics\n        metrics = self.metrics_collector.get_current_metrics()\n        \n        # Check component health\n        health_status = self.health_checker.check_all_components()\n        \n        # Log important events\n        self.log_manager.log_system_state(metrics, health_status)\n        \n        # Trigger alerts if necessary\n        self.evaluate_alerts(metrics, health_status)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational"}),": High-performance GPU for real-time inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensors"}),": Cameras, microphones, and tactile sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot"}),": Actuators and controllers compatible with the system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Communication"}),": Reliable network for remote operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"environmental-adaptations",children:"Environmental Adaptations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lighting conditions"}),": Adapting vision processing for different environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise levels"}),": Adjusting audio processing for different acoustic conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Workspace constraints"}),": Modifying action generation for physical limitations"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"simulation-testing",children:"Simulation Testing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Virtual environments for safe algorithm testing"}),"\n",(0,s.jsx)(n.li,{children:"Physics-accurate simulations for complex interactions"}),"\n",(0,s.jsx)(n.li,{children:"Stress testing with edge cases and failure scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-world-validation",children:"Real-World Validation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gradual deployment with human oversight"}),"\n",(0,s.jsx)(n.li,{children:"A/B testing of different VLA model configurations"}),"\n",(0,s.jsx)(n.li,{children:"Long-term reliability studies"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"example-complete-integration",children:"Example Complete Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class CompleteVLASystem:\n    def __init__(self):\n        # Initialize all VLA components\n        self.input_processor = InputProcessor()\n        self.whisper_integration = WhisperVoiceIntegration()\n        self.vla_model = load_pretrained_vla_model()\n        self.action_executor = ActionExecutor()\n        self.safety_validator = SafetyValidator()\n        self.system_monitor = SystemMonitor()\n        \n    def execute_command(self, audio_command, visual_context):\n        try:\n            # Step 1: Process voice command\n            text_command = self.whisper_integration.process_voice_command(\n                audio_command\n            )\n            \n            # Step 2: Fuse visual and language inputs\n            fused_representation = self.vla_model.encode(\n                visual_context, \n                text_command\n            )\n            \n            # Step 3: Generate action sequence\n            action_sequence = self.vla_model.decode(fused_representation)\n            \n            # Step 4: Validate safety\n            safety_check = self.safety_validator.validate_action(\n                action_sequence, \n                visual_context\n            )\n            \n            if not safety_check['safe']:\n                raise SafetyException(\n                    f\"Action unsafe: {safety_check['risk_score']}\"\n                )\n            \n            # Step 5: Execute action\n            execution_result = self.action_executor.execute(\n                action_sequence\n            )\n            \n            # Step 6: Monitor and log\n            self.system_monitor.monitor_system()\n            \n            return execution_result\n            \n        except Exception as e:\n            # Handle errors and return to safe state\n            self.action_executor.emergency_stop()\n            raise e\n"})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-and-improvement",children:"Evaluation and Improvement"}),"\n",(0,s.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Time"}),": Latency from command to action initiation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy"}),": Precision of executed actions compared to intended"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Incidents"}),": Number of safety-related interventions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"continuous-improvement",children:"Continuous Improvement"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Regular model retraining with new interaction data"}),"\n",(0,s.jsx)(n.li,{children:"User feedback integration for experience enhancement"}),"\n",(0,s.jsx)(n.li,{children:"A/B testing of new features and algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Systematic error analysis and correction"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deployment-pipeline",children:"Deployment Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Raw Sensor Data] --\x3e B[Preprocessing]\n    B --\x3e C[Whisper Integration]\n    C --\x3e D[VLA Model Processing]\n    D --\x3e E[Safety Validation]\n    E --\x3e F[Action Execution]\n    F --\x3e G[Feedback Collection]\n    G --\x3e H[System Monitoring]\n    H --\x3e I[Performance Analysis]\n    I --\x3e J[Model Retraining]\n    J --\x3e D\n"})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"The capstone logic represents the culmination of all VLA components into a functional embodied intelligence system. Success depends on careful integration of each component, robust safety mechanisms, and continuous monitoring and improvement. The system must be designed to handle real-world complexity while maintaining safety and reliability."}),"\n",(0,s.jsx)(n.p,{children:"This complete implementation provides a foundation for building sophisticated robotic applications that can understand natural language commands, perceive their environment, and execute appropriate actions in response."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8885:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(9378);const s={},a=t.createContext(s);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);