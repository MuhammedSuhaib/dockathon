"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[24],{45:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-04-vision-language-action/README","title":"Module 4: Vision-Language-Action (VLA) Models","description":"This directory contains the complete documentation for Module 4","source":"@site/docs/module-04-vision-language-action/README.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/README.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Logic: Complete VLA System Implementation","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/capstone-logic"},"next":{"title":"chapter-08-vla-models","permalink":"/SpecKit-Plus/docs/module-04-vision-language-action/chapter-08-vla-models"}}');var t=o(2714),s=o(8885);const l={},a="Module 4: Vision-Language-Action (VLA) Models",c={},d=[{value:"Files in this module:",id:"files-in-this-module",level:2},{value:"Purpose",id:"purpose",level:2}];function r(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla-models",children:"Module 4: Vision-Language-Action (VLA) Models"})}),"\n",(0,t.jsx)(n.p,{children:"This directory contains the complete documentation for Module 4: Vision-Language-Action Models in Embodied Intelligence. This module covers:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Whisper Voice Integration"}),"\n",(0,t.jsx)(n.li,{children:"VLA Models"}),"\n",(0,t.jsx)(n.li,{children:"Capstone Logic"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"files-in-this-module",children:"Files in this module:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"intro.md"})," - Overview of Vision-Language-Action models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"whisper-integration.md"})," - Integration of Whisper for voice processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"vla-models.md"})," - Detailed explanation of VLA model architectures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"capstone-logic.md"})," - Complete system implementation and integration"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"This module serves as a comprehensive guide to implementing Vision-Language-Action systems that enable robots and AI agents to perceive their environment, understand natural language commands, and execute physical actions."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(r,{...e})}):r(e)}},8885:(e,n,o)=>{o.d(n,{R:()=>l,x:()=>a});var i=o(9378);const t={},s=i.createContext(t);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);