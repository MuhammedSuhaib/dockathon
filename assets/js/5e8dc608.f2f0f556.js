"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[638],{5819:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Robotic Nervous System","items":[{"type":"link","href":"/SpecKit-Plus/docs/module-01-robotic-nervous-system/intro","label":"intro","docId":"module-01-robotic-nervous-system/intro","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-01-robotic-nervous-system/chapter-01-ros2-basics","label":"chapter-01-ros2-basics","docId":"module-01-robotic-nervous-system/chapter-01-ros2-basics","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-01-robotic-nervous-system/chapter-02-hardware","label":"chapter-02-hardware","docId":"module-01-robotic-nervous-system/chapter-02-hardware","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Digital Twin","items":[{"type":"link","href":"/SpecKit-Plus/docs/module-02-digital-twin/intro","label":"intro","docId":"module-02-digital-twin/intro","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-02-digital-twin/chapter-03-gazebo-urdf","label":"chapter-03-gazebo-urdf","docId":"module-02-digital-twin/chapter-03-gazebo-urdf","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-02-digital-twin/chapter-04-unity","label":"chapter-04-unity","docId":"module-02-digital-twin/chapter-04-unity","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"AI Robot Brain","items":[{"type":"link","href":"/SpecKit-Plus/docs/module-03-ai-robot-brain/intro","label":"intro","docId":"module-03-ai-robot-brain/intro","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-05-computer-vision","label":"chapter-05-computer-vision","docId":"module-03-ai-robot-brain/chapter-05-computer-vision","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-06-navigation","label":"chapter-06-navigation","docId":"module-03-ai-robot-brain/chapter-06-navigation","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-07-isaac-sim","label":"chapter-07-isaac-sim","docId":"module-03-ai-robot-brain/chapter-07-isaac-sim","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-03-ai-robot-brain/chapter-08-isaac-ros","label":"chapter-08-isaac-ros","docId":"module-03-ai-robot-brain/chapter-08-isaac-ros","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Vision Language Action","items":[{"type":"link","href":"/SpecKit-Plus/docs/module-04-vision-language-action/intro","label":"intro","docId":"module-04-vision-language-action/intro","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-04-vision-language-action/chapter-07-whisper","label":"chapter-07-whisper","docId":"module-04-vision-language-action/chapter-07-whisper","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-04-vision-language-action/chapter-08-vla-models","label":"chapter-08-vla-models","docId":"module-04-vision-language-action/chapter-08-vla-models","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-04-vision-language-action/capstone-logic","label":"capstone-logic","docId":"module-04-vision-language-action/capstone-logic","unlisted":false},{"type":"link","href":"/SpecKit-Plus/docs/module-04-vision-language-action/chapter-09-capstone-project","label":"chapter-09-capstone-project","docId":"module-04-vision-language-action/chapter-09-capstone-project","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module-01-robotic-nervous-system/chapter-01-ros2-basics":{"id":"module-01-robotic-nervous-system/chapter-01-ros2-basics","title":"chapter-01-ros2-basics","description":"ROS 2 (Robot Operating System 2) provides the communication framework for robotic applications. Understanding its core concepts of nodes, topics, and services is essential for building distributed robotic systems.","sidebar":"tutorialSidebar"},"module-01-robotic-nervous-system/chapter-02-hardware":{"id":"module-01-robotic-nervous-system/chapter-02-hardware","title":"chapter-02-hardware","description":"URDF (Unified Robot Description Format) is essential for defining humanoid robot models. This chapter covers both URDF for humanoids and hardware setup with Jetson Orin and RealSense cameras.","sidebar":"tutorialSidebar"},"module-01-robotic-nervous-system/intro":{"id":"module-01-robotic-nervous-system/intro","title":"intro","description":"From Digital AI to Physical AI","sidebar":"tutorialSidebar"},"module-02-digital-twin/chapter-03-gazebo-urdf":{"id":"module-02-digital-twin/chapter-03-gazebo-urdf","title":"chapter-03-gazebo-urdf","description":"Unified Robot Description Format (URDF) is an XML-based format used to describe robotic systems in ROS. It defines the physical and visual properties of a robot, including links, joints, and their relationships.","sidebar":"tutorialSidebar"},"module-02-digital-twin/chapter-04-unity":{"id":"module-02-digital-twin/chapter-04-unity","title":"chapter-04-unity","description":"Unity is a powerful real-time 3D development platform that excels in creating high-fidelity visualizations and human-robot interaction environments. This chapter explores how Unity can be integrated with robotics for advanced simulation and visualization.","sidebar":"tutorialSidebar"},"module-02-digital-twin/intro":{"id":"module-02-digital-twin/intro","title":"intro","description":"Why we simulate","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-05-computer-vision":{"id":"module-03-ai-robot-brain/chapter-05-computer-vision","title":"chapter-05-computer-vision","description":"Introduction to Computer Vision in Robotics","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-06-navigation":{"id":"module-03-ai-robot-brain/chapter-06-navigation","title":"chapter-06-navigation","description":"Introduction to Humanoid Navigation","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-07-isaac-sim":{"id":"module-03-ai-robot-brain/chapter-07-isaac-sim","title":"chapter-07-isaac-sim","description":"NVIDIA Isaac Sim provides a comprehensive simulation environment for robotics development, specifically designed for training humanoid robots. It uses Universal Scene Description (USD) as its core data format, enabling high-fidelity physics simulation, photorealistic rendering, and large-scale synthetic data generation for embodied AI applications.","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-08-isaac-ros":{"id":"module-03-ai-robot-brain/chapter-08-isaac-ros","title":"chapter-08-isaac-ros","description":"Isaac ROS is a collection of hardware-accelerated perception and navigation packages designed to run on NVIDIA robotics platforms. This chapter explores how Isaac ROS enables efficient Visual SLAM (VSLAM) and navigation on robotic platforms.","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/intro":{"id":"module-03-ai-robot-brain/intro","title":"intro","description":"AI Robot Brain - Perception Overview","sidebar":"tutorialSidebar"},"module-04-vision-language-action/capstone-logic":{"id":"module-04-vision-language-action/capstone-logic","title":"capstone-logic","description":"This chapter provides implementation guidance for creating the complete Autonomous Humanoid system that integrates all the components learned throughout the course. This serves as a practical guide to implement the capstone project described in the previous chapter.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/chapter-07-whisper":{"id":"module-04-vision-language-action/chapter-07-whisper","title":"chapter-07-whisper","description":"This chapter explores the integration of OpenAI Whisper for speech recognition combined with ROS 2 for robotic control, creating a voice-to-action pipeline that enables natural human-robot interaction. This system serves as the foundation for translating human voice commands into robotic actions.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/chapter-08-vla-models":{"id":"module-04-vision-language-action/chapter-08-vla-models","title":"chapter-08-vla-models","description":"Vision-Language-Action (VLA) models represent a breakthrough in embodied AI, enabling robots to interpret natural language commands and execute appropriate actions in physical environments. This chapter explores cognitive planning systems that translate high-level language instructions into sequences of robotic actions.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/chapter-09-capstone-project":{"id":"module-04-vision-language-action/chapter-09-capstone-project","title":"chapter-09-capstone-project","description":"This capstone project brings together all concepts learned throughout the course to create an autonomous humanoid robot capable of receiving voice commands, planning paths, navigating obstacles, identifying objects using computer vision, and manipulating them. This project demonstrates the integration of ROS 2, Gazebo simulation, NVIDIA Isaac, and Vision-Language-Action (VLA) systems.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/intro":{"id":"module-04-vision-language-action/intro","title":"intro","description":"Vision-Language-Action (VLA) - Overview","sidebar":"tutorialSidebar"}}}}')}}]);