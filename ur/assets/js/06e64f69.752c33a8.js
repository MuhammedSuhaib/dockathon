"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[477],{676:(e,n,t)=>{t.d(n,{A:()=>o});var a=t(9378),i=t(2714);function o(){const[e,n]=(0,a.useState)(!1),[t,o]=(0,a.useState)(null),r=(e,n,t)=>{if(0===e.children.length)return void(e.textContent=t);const a=document.createTreeWalker(e,NodeFilter.SHOW_TEXT,null),i=[];let o;for(;o=a.nextNode();)i.push(o);i.forEach(e=>{e.textContent?.trim()===n.trim()?e.textContent=t:e.textContent?.includes(n)&&(e.textContent=e.textContent?.replace(n,t)||"")})};return(0,i.jsxs)("div",{className:"translate-button-container",style:{marginBottom:"20px"},children:[(0,i.jsx)("button",{onClick:async()=>{if(!e)if("undefined"!=typeof window){n(!0),o(null);try{const e=document.querySelector("article");if(!e)throw new Error("Article content not found");const t=e.querySelectorAll("div, p, h1, h2, h3, h4, h5, h6, span, li"),a=Array.from(t).map(e=>({element:e,originalText:e.textContent||"",isCode:null!==e.closest("pre, code")})).filter(e=>""!==e.originalText.trim()&&!e.isCode);if(0===a.length)throw new Error("No translatable content found");if(!confirm("This will translate the content to Urdu. Auto-translated content may have errors. Continue?"))return void n(!1);for(const n of a)if(""!==n.originalText.trim()&&!n.isCode){const e=await fetch("http://localhost:8000/api/translate-text",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:n.originalText,target_language:"ur"})});if(!e.ok)throw new Error(`Translation API error: ${e.status}`);const{translated_text:t}=await e.json();n.element.setAttribute("dir","rtl"),n.element.style.direction="rtl",n.element.style.textAlign="right",n.element.style.fontFamily="Tahoma, Arial, sans-serif",r(n.element,n.originalText,t)}alert("Translation to Urdu completed!")}catch(t){console.error("Translation error:",t),o(t instanceof Error?t.message:"An error occurred during translation")}finally{n(!1)}}else o("Translation is only available in browser environment")},disabled:e,style:{padding:"8px 16px",backgroundColor:e?"#ccc":"#00cc44",color:"white",border:"none",borderRadius:"4px",cursor:e?"not-allowed":"pointer"},children:e?"Translating...":".Translate to Urdu"}),t&&(0,i.jsxs)("div",{style:{color:"red",marginTop:"5px"},children:["Error: ",t]})]})}},8885:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var a=t(9378);const i={},o=a.createContext(i);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(o.Provider,{value:n},e.children)}},9830:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-04-vision-language-action/chapter-08-vla-models","title":"chapter-08-vla-models","description":"Vision-Language-Action (VLA) models represent a breakthrough in embodied AI, enabling robots to interpret natural language commands and execute appropriate actions in physical environments. This chapter explores cognitive planning systems that translate high-level language instructions into sequences of robotic actions.","source":"@site/docs/module-04-vision-language-action/chapter-08-vla-models.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/chapter-08-vla-models","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/chapter-08-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/chapter-08-vla-models.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"chapter-07-whisper","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/chapter-07-whisper"},"next":{"title":"capstone-logic","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/capstone-logic"}}');var i=t(2714),o=t(8885),r=t(676);const s={sidebar_position:4},l="Cognitive Planning with VLA Models - From Language to Action",c={},d=[{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:2},{value:"Planning Pipeline",id:"planning-pipeline",level:3},{value:"Task Decomposition",id:"task-decomposition",level:3},{value:"Translating Natural Language to ROS 2 Actions",id:"translating-natural-language-to-ros-2-actions",level:2},{value:"Semantic Command Mapping",id:"semantic-command-mapping",level:3},{value:"Language Understanding for Robotics",id:"language-understanding-for-robotics",level:2},{value:"Natural Language Processing Pipeline",id:"natural-language-processing-pipeline",level:3},{value:"Named Entity Recognition for Robotics",id:"named-entity-recognition-for-robotics",level:4},{value:"Spatial Language Understanding",id:"spatial-language-understanding",level:4},{value:"ROS 2 Action Sequencing",id:"ros-2-action-sequencing",level:2},{value:"Converting Language to ROS 2 Services",id:"converting-language-to-ros-2-services",level:3},{value:"Vision-Language Integration in Planning",id:"vision-language-integration-in-planning",level:2},{value:"Scene Understanding for Action Planning",id:"scene-understanding-for-action-planning",level:3},{value:"Real-World Execution Challenges",id:"real-world-execution-challenges",level:2},{value:"Handling Ambiguity",id:"handling-ambiguity",level:3},{value:"Feedback Integration",id:"feedback-integration",level:3},{value:"Case Study: &quot;Clean the Room&quot; Implementation",id:"case-study-clean-the-room-implementation",level:2},{value:"1. Language Understanding",id:"1-language-understanding",level:3},{value:"2. Task Decomposition",id:"2-task-decomposition",level:3},{value:"3. ROS 2 Action Generation",id:"3-ros-2-action-generation",level:3},{value:"Evaluation Metrics for Cognitive Planning",id:"evaluation-metrics-for-cognitive-planning",level:2},{value:"Success Metrics",id:"success-metrics",level:3},{value:"Quality Metrics",id:"quality-metrics",level:3},{value:"Future Directions",id:"future-directions",level:2},{value:"Summary",id:"summary",level:2}];function g(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.A,{}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"cognitive-planning-with-vla-models---from-language-to-action",children:"Cognitive Planning with VLA Models - From Language to Action"})}),"\n",(0,i.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent a breakthrough in embodied AI, enabling robots to interpret natural language commands and execute appropriate actions in physical environments. This chapter explores cognitive planning systems that translate high-level language instructions into sequences of robotic actions."}),"\n",(0,i.jsx)(n.h2,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Cognitive planning in VLA systems involves translating complex natural language commands into executable robotic actions through multi-step reasoning."}),"\n",(0,i.jsx)(n.h3,{id:"planning-pipeline",children:"Planning Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Natural Language ("Clean the room")\n    \u2193\nLanguage Understanding \u2192 Task Decomposition \u2192 Action Sequencing \u2192 Execution\n    \u2193                        \u2193                      \u2193                   \u2193\nSemantic Parsing        Subtask Generation    Trajectory Planning   Robot Control\n'})}),"\n",(0,i.jsx)(n.h3,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CognitivePlanner:\n    def __init__(self):\n        self.language_interpreter = LanguageInterpreter()\n        self.task_decomposer = TaskDecomposer()\n        self.action_generator = ActionGenerator()\n\n    def plan_from_language(self, command: str, environment_state: dict):\n        # 1. Parse natural language command\n        semantic_intent = self.language_interpreter.parse(command)\n\n        # 2. Decompose into subtasks\n        subtasks = self.task_decomposer.decompose(semantic_intent, environment_state)\n\n        # 3. Generate sequence of actions\n        action_sequence = self.action_generator.generate(subtasks, environment_state)\n\n        return action_sequence\n\n# Example usage\nplanner = CognitivePlanner()\ncommand = "Clean the room"\nactions = planner.plan_from_language(command, current_env_state)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"translating-natural-language-to-ros-2-actions",children:"Translating Natural Language to ROS 2 Actions"}),"\n",(0,i.jsx)(n.p,{children:'The core challenge in cognitive planning is converting natural language like "Clean the room" into specific ROS 2 action sequences:'}),"\n",(0,i.jsx)(n.h3,{id:"semantic-command-mapping",children:"Semantic Command Mapping"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SemanticCommandMapper:\n    def __init__(self):\n        self.command_patterns = {\n            "clean_room": [\n                "clean", "tidy", "organize", "pick up", "put away"\n            ],\n            "fetch_object": [\n                "bring me", "get", "fetch", "hand me", "go get"\n            ],\n            "navigate_to": [\n                "go to", "move to", "walk to", "travel to", "reach"\n            ]\n        }\n\n    def map_command_to_tasks(self, natural_language: str):\n        # Identify command type from natural language\n        command_type = self.identify_command_type(natural_language)\n\n        # Generate task-specific plan\n        if command_type == "clean_room":\n            return self.generate_cleaning_plan(natural_language)\n        elif command_type == "fetch_object":\n            return self.generate_fetch_plan(natural_language)\n        elif command_type == "navigate_to":\n            return self.generate_navigation_plan(natural_language)\n\ndef generate_cleaning_plan(self, command: str):\n    # Example: "Clean the room" -> sequence of cleaning tasks\n    tasks = [\n        {"action": "identify_objects", "target": "floor"},\n        {"action": "detect_debris", "target": "living_room"},\n        {"action": "plan_path", "target": "debris_location"},\n        {"action": "navigate", "target": "debris_location"},\n        {"action": "grasp", "target": "debris_object"},\n        {"action": "dispose", "target": "waste_bin"},\n        {"action": "check_completion", "target": "room"}\n    ]\n    return tasks\n'})}),"\n",(0,i.jsx)(n.h2,{id:"language-understanding-for-robotics",children:"Language Understanding for Robotics"}),"\n",(0,i.jsx)(n.h3,{id:"natural-language-processing-pipeline",children:"Natural Language Processing Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"VLA models integrate advanced NLP techniques specifically for robotics:"}),"\n",(0,i.jsx)(n.h4,{id:"named-entity-recognition-for-robotics",children:"Named Entity Recognition for Robotics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Object recognition: "the red ball", "that book", "the chair"'}),"\n",(0,i.jsx)(n.li,{children:'Spatial relationships: "on the table", "under the chair", "next to the door"'}),"\n",(0,i.jsx)(n.li,{children:'Action targets: "move it", "pick that up", "put it there"'}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"spatial-language-understanding",children:"Spatial Language Understanding"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SpatialLanguageProcessor:\n    def __init__(self):\n        self.spatial_relations = ["on", "under", "next_to", "between", "behind", "in_front_of"]\n        self.object_detectors = ObjectDetectionSystem()\n\n    def parse_spatial_command(self, command: str):\n        # Parse "Put the book on the table"\n        entities = self.extract_entities(command)  # {"book": "object", "table": "location"}\n        relations = self.extract_relationships(command)  # {"on": "spatial_relation"}\n\n        # Generate ROS 2 actions\n        actions = self.generate_spatial_actions(entities, relations)\n        return actions\n'})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-action-sequencing",children:"ROS 2 Action Sequencing"}),"\n",(0,i.jsx)(n.h3,{id:"converting-language-to-ros-2-services",children:"Converting Language to ROS 2 Services"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Translating "Fetch the red cup from the kitchen and bring it to me"\nclass LanguageToROSConverter:\n    def convert_to_ros_actions(self, parsed_command):\n        ros_actions = []\n\n        # Step 1: Navigate to kitchen\n        ros_actions.append({\n            "service": "/navigate_to_pose",\n            "parameters": {"x": kitchen_x, "y": kitchen_y, "theta": 0.0}\n        })\n\n        # Step 2: Detect red cup\n        ros_actions.append({\n            "service": "/object_detection/detect",\n            "parameters": {"object_type": "cup", "color": "red"}\n        })\n\n        # Step 3: Grasp the cup\n        ros_actions.append({\n            "service": "/manipulator/grasp",\n            "parameters": {"object_pose": detected_pose}\n        })\n\n        # Step 4: Navigate back to user\n        ros_actions.append({\n            "service": "/navigate_to_pose",\n            "parameters": {"x": user_x, "y": user_y, "theta": user_theta}\n        })\n\n        # Step 5: Release the cup\n        ros_actions.append({\n            "service": "/manipulator/release",\n            "parameters": {}\n        })\n\n        return ros_actions\n'})}),"\n",(0,i.jsx)(n.h2,{id:"vision-language-integration-in-planning",children:"Vision-Language Integration in Planning"}),"\n",(0,i.jsx)(n.h3,{id:"scene-understanding-for-action-planning",children:"Scene Understanding for Action Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class VisionLanguagePlanner:\n    def __init__(self):\n        self.vision_system = PerceptionSystem()\n        self.language_model = VLAModel()\n        self.action_executor = ROS2ActionExecutor()\n\n    def execute_language_command(self, command: str):\n        # Get current scene understanding\n        scene_description = self.vision_system.get_scene_description()\n\n        # Combine with language command to generate plan\n        action_plan = self.language_model.plan_from_language_and_vision(\n            command,\n            scene_description\n        )\n\n        # Execute plan with ROS 2\n        for action in action_plan:\n            self.action_executor.execute(action)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"real-world-execution-challenges",children:"Real-World Execution Challenges"}),"\n",(0,i.jsx)(n.h3,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,i.jsx)(n.p,{children:"Natural language commands often contain ambiguities that need resolution:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"That book" - requires visual reference resolution'}),"\n",(0,i.jsx)(n.li,{children:'"Over there" - requires spatial reference resolution'}),"\n",(0,i.jsx)(n.li,{children:'"Right now" - requires timing interpretation'}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"feedback-integration",children:"Feedback Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AdaptiveCognitivePlanner:\n    def execute_with_feedback(self, command: str):\n        plan = self.plan_from_language(command)\n\n        for i, action in enumerate(plan):\n            try:\n                result = self.execute_action(action)\n\n                # Update plan based on result\n                if not result.success:\n                    # Adjust plan based on failure\n                    adjusted_plan = self.revise_plan(plan, i, result.error)\n                    return self.execute_with_feedback(command)  # Recursive attempt\n\n            except Exception as e:\n                # Handle execution errors\n                print(f"Action failed: {e}")\n                return False\n\n        return True\n'})}),"\n",(0,i.jsx)(n.h2,{id:"case-study-clean-the-room-implementation",children:'Case Study: "Clean the Room" Implementation'}),"\n",(0,i.jsx)(n.p,{children:'Let\'s walk through how a complex command like "Clean the room" gets processed:'}),"\n",(0,i.jsx)(n.h3,{id:"1-language-understanding",children:"1. Language Understanding"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Command: "Clean the room"'}),"\n",(0,i.jsx)(n.li,{children:"Identified intent: room cleaning operation"}),"\n",(0,i.jsx)(n.li,{children:"Target area: current room/entire space"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-task-decomposition",children:"2. Task Decomposition"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Detect objects that need cleaning"}),"\n",(0,i.jsx)(n.li,{children:"Categorize objects (trash vs. misplaced items)"}),"\n",(0,i.jsx)(n.li,{children:"Plan cleaning sequence"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-ros-2-action-generation",children:"3. ROS 2 Action Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def clean_room_plan():\n    actions = [\n        # Scan room for objects\n        {"service": "/navigation/scan_room", "params": {}},\n\n        # Find and approach first debris item\n        {"service": "/navigation/move_to", "params": {"x": debris_x, "y": debris_y}},\n\n        # Grasp debris\n        {"service": "/manipulation/grasp", "params": {"object_id": "debris_1"}},\n\n        # Dispose in bin\n        {"service": "/navigation/move_to", "params": {"x": bin_x, "y": bin_y}},\n        {"service": "/manipulation/release", "params": {}},\n\n        # Return to search for next item\n        {"service": "/navigation/return_to_search", "params": {}},\n\n        # Repeat until room is clean\n    ]\n    return actions\n'})}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-metrics-for-cognitive-planning",children:"Evaluation Metrics for Cognitive Planning"}),"\n",(0,i.jsx)(n.h3,{id:"success-metrics",children:"Success Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task completion rate"}),"\n",(0,i.jsx)(n.li,{children:"Language understanding accuracy"}),"\n",(0,i.jsx)(n.li,{children:"Action success rate"}),"\n",(0,i.jsx)(n.li,{children:"Planning efficiency"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"quality-metrics",children:"Quality Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Number of retries needed"}),"\n",(0,i.jsx)(n.li,{children:"Time to complete tasks"}),"\n",(0,i.jsx)(n.li,{children:"Safety violations"}),"\n",(0,i.jsx)(n.li,{children:"User satisfaction"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,i.jsx)(n.p,{children:"Cognitive planning continues to evolve with:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"More sophisticated language models"}),"\n",(0,i.jsx)(n.li,{children:"Better integration of world models"}),"\n",(0,i.jsx)(n.li,{children:"Improved multi-step reasoning"}),"\n",(0,i.jsx)(n.li,{children:"Enhanced adaptability to new environments"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Cognitive planning in VLA systems bridges natural language understanding with robotic action execution. By breaking down high-level commands into sequences of ROS 2 actions, these systems enable natural human-robot interaction and make complex robotic tasks accessible through everyday language."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}}}]);