"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[854],{676:(e,n,i)=>{i.d(n,{A:()=>r});var t=i(9378),o=i(2714);function r(){const[e,n]=(0,t.useState)(!1),[i,r]=(0,t.useState)(null),s=(e,n,i)=>{if(0===e.children.length)return void(e.textContent=i);const t=document.createTreeWalker(e,NodeFilter.SHOW_TEXT,null),o=[];let r;for(;r=t.nextNode();)o.push(r);o.forEach(e=>{e.textContent?.trim()===n.trim()?e.textContent=i:e.textContent?.includes(n)&&(e.textContent=e.textContent?.replace(n,i)||"")})};return(0,o.jsxs)("div",{className:"translate-button-container",style:{marginBottom:"20px"},children:[(0,o.jsx)("button",{onClick:async()=>{if(!e)if("undefined"!=typeof window){n(!0),r(null);try{const e=document.querySelector("article");if(!e)throw new Error("Article content not found");const i=e.querySelectorAll("div, p, h1, h2, h3, h4, h5, h6, span, li"),t=Array.from(i).map(e=>({element:e,originalText:e.textContent||"",isCode:null!==e.closest("pre, code")})).filter(e=>""!==e.originalText.trim()&&!e.isCode);if(0===t.length)throw new Error("No translatable content found");if(!confirm("This will translate the content to Urdu. Auto-translated content may have errors. Continue?"))return void n(!1);for(const n of t)if(""!==n.originalText.trim()&&!n.isCode){const e=await fetch("http://localhost:8000/api/translate-text",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:n.originalText,target_language:"ur"})});if(!e.ok)throw new Error(`Translation API error: ${e.status}`);const{translated_text:i}=await e.json();n.element.setAttribute("dir","rtl"),n.element.style.direction="rtl",n.element.style.textAlign="right",n.element.style.fontFamily="Tahoma, Arial, sans-serif",s(n.element,n.originalText,i)}alert("Translation to Urdu completed!")}catch(i){console.error("Translation error:",i),r(i instanceof Error?i.message:"An error occurred during translation")}finally{n(!1)}}else r("Translation is only available in browser environment")},disabled:e,style:{padding:"8px 16px",backgroundColor:e?"#ccc":"#00cc44",color:"white",border:"none",borderRadius:"4px",cursor:e?"not-allowed":"pointer"},children:e?"Translating...":".Translate to Urdu"}),i&&(0,o.jsxs)("div",{style:{color:"red",marginTop:"5px"},children:["Error: ",i]})]})}},1042:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-04-vision-language-action/chapter-07-whisper","title":"chapter-07-whisper","description":"This chapter explores the integration of OpenAI Whisper for speech recognition combined with ROS 2 for robotic control, creating a voice-to-action pipeline that enables natural human-robot interaction. This system serves as the foundation for translating human voice commands into robotic actions.","source":"@site/docs/module-04-vision-language-action/chapter-07-whisper.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/chapter-07-whisper","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/chapter-07-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/chapter-07-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"intro","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/intro"},"next":{"title":"chapter-08-vla-models","permalink":"/SpecKit-Plus/ur/docs/module-04-vision-language-action/chapter-08-vla-models"}}');var o=i(2714),r=i(8885),s=i(676);const a={sidebar_position:2},c="Voice-to-Action: Using OpenAI Whisper for Voice Commands",l={},d=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"OpenAI Whisper for Speech Recognition",id:"openai-whisper-for-speech-recognition",level:2},{value:"Whisper Model Architecture",id:"whisper-model-architecture",level:3},{value:"Whisper in Real-time Robotics Applications",id:"whisper-in-real-time-robotics-applications",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"ROS 2 Node Architecture",id:"ros-2-node-architecture",level:3},{value:"Real-time Processing Considerations",id:"real-time-processing-considerations",level:3},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:2},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Command Mapping",id:"command-mapping",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Audio Quality in Robotic Environments",id:"audio-quality-in-robotic-environments",level:3},{value:"Processing Latency",id:"processing-latency",level:3},{value:"Multilingual Support",id:"multilingual-support",level:3},{value:"Advanced Integration Techniques",id:"advanced-integration-techniques",level:2},{value:"Context-Aware Processing",id:"context-aware-processing",level:3},{value:"Confidence Thresholding",id:"confidence-thresholding",level:3},{value:"ROS 2 Message Specifications",id:"ros-2-message-specifications",level:2},{value:"Security Considerations",id:"security-considerations",level:2},{value:"Future Directions",id:"future-directions",level:2},{value:"Summary",id:"summary",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(s.A,{}),"\n",(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-using-openai-whisper-for-voice-commands",children:"Voice-to-Action: Using OpenAI Whisper for Voice Commands"})}),"\n",(0,o.jsx)(n.p,{children:"This chapter explores the integration of OpenAI Whisper for speech recognition combined with ROS 2 for robotic control, creating a voice-to-action pipeline that enables natural human-robot interaction. This system serves as the foundation for translating human voice commands into robotic actions."}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-Action systems bridge the gap between natural human language and robotic execution. By leveraging OpenAI's Whisper for speech recognition and ROS 2 for robotic control, we create a system that can understand spoken commands and execute appropriate actions. This is a critical component of the Physical AI ecosystem, allowing humans to interact with robots using natural language."}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-for-speech-recognition",children:"OpenAI Whisper for Speech Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a state-of-the-art automatic speech recognition (ASR) system that converts speech to text. In the context of robotics, Whisper serves as the initial processing layer that converts human voice commands into text format for further processing."}),"\n",(0,o.jsx)(n.h3,{id:"whisper-model-architecture",children:"Whisper Model Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is built on a transformer-based architecture that can handle multiple languages and various audio conditions. Key features include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Multilingual support for over 99 languages"}),"\n",(0,o.jsx)(n.li,{children:"Robustness to accents, background noise, and technical speech"}),"\n",(0,o.jsx)(n.li,{children:"Support for both transcription and translation"}),"\n",(0,o.jsx)(n.li,{children:"Different model sizes to balance accuracy and computational requirements"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"whisper-in-real-time-robotics-applications",children:"Whisper in Real-time Robotics Applications"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\nimport rospy\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\nclass WhisperVoiceToAction:\n    def __init__(self):\n        # Initialize Whisper model\n        self.model = whisper.load_model("base")\n        \n        # Initialize ROS 2 node\n        rospy.init_node(\'whisper_voice_to_action\')\n        \n        # Subscriber for audio data\n        self.audio_sub = rospy.Subscriber(\'/audio\', AudioData, self.audio_callback)\n        \n        # Publisher for recognized text\n        self.text_pub = rospy.Publisher(\'/recognized_text\', String, queue_size=10)\n        \n        # Publisher for robot commands\n        self.command_pub = rospy.Publisher(\'/robot_command\', String, queue_size=10)\n        \n        rospy.loginfo("Whisper Voice-to-Action node initialized")\n    \n    def audio_callback(self, audio_msg):\n        # Convert audio data to format suitable for Whisper\n        audio_array = self.convert_audio_msg_to_array(audio_msg)\n        \n        # Transcribe audio to text\n        result = self.model.transcribe(audio_array)\n        text = result["text"]\n        \n        # Publish recognized text\n        self.text_pub.publish(String(data=text))\n        \n        # Process text command and generate robot action\n        self.process_command(text)\n    \n    def process_command(self, text):\n        # Extract command from recognized text\n        command = self.extract_command(text)\n        \n        if command:\n            # Publish command to robot\n            self.command_pub.publish(String(data=command))\n            rospy.loginfo(f"Command sent to robot: {command}")\n    \n    def extract_command(self, text):\n        # Simple command extraction (can be enhanced with NLP techniques)\n        text = text.lower().strip()\n        \n        # Define possible robot commands\n        if "move forward" in text:\n            return "move_forward"\n        elif "move backward" in text:\n            return "move_backward"\n        elif "turn left" in text:\n            return "turn_left"\n        elif "turn right" in text:\n            return "turn_right"\n        elif "stop" in text:\n            return "stop"\n        elif "pick up" in text or "grasp" in text:\n            return "pick_object"\n        elif "place" in text or "put" in text:\n            return "place_object"\n        else:\n            rospy.logwarn(f"Unknown command: {text}")\n            return None\n    \n    def convert_audio_msg_to_array(self, audio_msg):\n        # Convert ROS audio message to numpy array for Whisper\n        import numpy as np\n        audio_array = np.frombuffer(audio_msg.data, dtype=np.int16)\n        return audio_array.astype(np.float32) / 32768.0  # Normalize to [-1, 1]\n\nif __name__ == \'__main__\':\n    try:\n        voice_to_action = WhisperVoiceToAction()\n        rospy.spin()\n    except rospy.ROSInterruptException:\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.p,{children:"ROS 2 (Robot Operating System 2) provides the middleware for robotic communication and control. The integration between Whisper and ROS 2 enables:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Real-time audio streaming from robot microphones"}),"\n",(0,o.jsx)(n.li,{children:"Text publishing for further NLP processing"}),"\n",(0,o.jsx)(n.li,{children:"Command execution on robotic platforms"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-node-architecture",children:"ROS 2 Node Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Audio Input \u2192 ROS 2 Subscriber \u2192 Whisper Processing \u2192 Command Publisher \u2192 Robot Action\n"})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-processing-considerations",children:"Real-time Processing Considerations"}),"\n",(0,o.jsx)(n.p,{children:"When implementing Whisper for real-time robotics:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Optimization"}),": Use smaller Whisper models for faster inference"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Streaming"}),": Implement efficient audio streaming to minimize processing delay"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Activity Detection"}),": Implement VAD to reduce unnecessary processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Optimize GPU/CPU usage for simultaneous processing"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The complete voice-to-action pipeline consists of:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Audio capture from robot's microphones"}),"\n",(0,o.jsx)(n.li,{children:"Real-time speech recognition using Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Natural language processing to extract intent"}),"\n",(0,o.jsx)(n.li,{children:"Command mapping to robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Execution of actions via ROS 2"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy import signal\n\nclass AudioPreprocessor:\n    def __init__(self):\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.frame_size = 1024\n        self.hop_length = 512\n        \n    def preprocess_audio(self, raw_audio, original_sample_rate):\n        # Resample to 16kHz if needed\n        if original_sample_rate != self.sample_rate:\n            num_samples = int(len(raw_audio) * self.sample_rate / original_sample_rate)\n            raw_audio = signal.resample(raw_audio, num_samples)\n        \n        # Normalize audio to [-1, 1]\n        raw_audio = raw_audio.astype(np.float32) / 32768.0\n        \n        return raw_audio\n"})}),"\n",(0,o.jsx)(n.h3,{id:"command-mapping",children:"Command Mapping"}),"\n",(0,o.jsx)(n.p,{children:"The mapping from recognized text to robot commands can be implemented with various techniques:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Rule-based mapping (keyword matching)"}),"\n",(0,o.jsx)(n.li,{children:"Template-based understanding"}),"\n",(0,o.jsx)(n.li,{children:"Natural language processing with LLMs"}),"\n",(0,o.jsx)(n.li,{children:"Intent classification models"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,o.jsx)(n.h3,{id:"audio-quality-in-robotic-environments",children:"Audio Quality in Robotic Environments"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Challenge"}),": Background noise and audio quality in real-world environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Use beamforming microphones and noise reduction algorithms"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"processing-latency",children:"Processing Latency"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Challenge"}),": Real-time requirements for responsive interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Model optimization and edge computing deployment"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"multilingual-support",children:"Multilingual Support"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Challenge"}),": Commands in different languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Use multilingual Whisper models with language detection"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"advanced-integration-techniques",children:"Advanced Integration Techniques"}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-processing",children:"Context-Aware Processing"}),"\n",(0,o.jsx)(n.p,{children:"Enhancing voice-to-action systems with contextual information:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def process_command_with_context(self, text, robot_state, environment_data):\n    # Combine recognized text with robot state and environmental context\n    command = self.extract_command(text)\n    \n    # Augment command with context-specific parameters\n    if command == "pick_object" and environment_data.closest_object:\n        # Include object details in command\n        command = f"pick_object:{environment_data.closest_object.type}"\n    \n    return command\n'})}),"\n",(0,o.jsx)(n.h3,{id:"confidence-thresholding",children:"Confidence Thresholding"}),"\n",(0,o.jsx)(n.p,{children:"Implementing confidence checks to improve reliability:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def process_with_confidence(self, audio):\n    result = self.model.transcribe(audio, return_dict=True)\n    text = result["text"]\n    avg_logprob = result["avg_logprob"]\n    \n    # Only process commands with sufficient confidence\n    if avg_logprob > -0.5:  # Threshold can be tuned\n        self.process_command(text)\n    else:\n        rospy.logwarn("Low confidence transcription, ignoring command")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-message-specifications",children:"ROS 2 Message Specifications"}),"\n",(0,o.jsx)(n.p,{children:"For integration with ROS 2, define appropriate message types:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Audio data: ",(0,o.jsx)(n.code,{children:"audio_common_msgs/AudioData"})," or custom message"]}),"\n",(0,o.jsxs)(n.li,{children:["Recognized text: ",(0,o.jsx)(n.code,{children:"std_msgs/String"})]}),"\n",(0,o.jsx)(n.li,{children:"Robot commands: Custom message types based on robot capabilities"}),"\n",(0,o.jsxs)(n.li,{children:["Robot state: ",(0,o.jsx)(n.code,{children:"nav_msgs/Odometry"})," or other appropriate messages"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,o.jsx)(n.p,{children:"When implementing voice-to-action systems:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Validate and sanitize recognized text to prevent injection attacks"}),"\n",(0,o.jsx)(n.li,{children:"Implement authentication for critical commands"}),"\n",(0,o.jsx)(n.li,{children:"Use encrypted communication for sensitive applications"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action systems for robotics continue to evolve with:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Improved real-time processing capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Better noise robustness for real-world environments"}),"\n",(0,o.jsx)(n.li,{children:"Integration with multimodal perception systems"}),"\n",(0,o.jsx)(n.li,{children:"Enhanced contextual understanding"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter covered the integration of OpenAI Whisper with ROS 2 to create voice-to-action systems for robotics. The combination enables natural human-robot interaction through speech recognition and robotic control. Key implementation considerations include real-time processing, audio quality, and command mapping for reliable operation."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},8885:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(9378);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);